import tensorflow as tf

# 데이터가 많은 경우 딥러닝으로 문제푸는법

#이전에 했던 것은 신발사이즈값 1개와 키값 1개만 있었음.. 이번엔 여러 데이터가 있을때 딥러닝으로 어떻게 학습시키는지 볼거임

train_x = [1,2,3,4,5,6,7]
train_y = [3,5,7,9,11,13,15]

"""
문제:  x에다가 뭔짓을 하면 y가 나올까요? (x와 y는 어떤 관계일까요)


@ 딥러닝하는 순서           ->  근데 이 방법은 현업에선 잘 안씀. 대학시험에서 나오거나, 박사과정에서 이거보다 더 효율적인 딥러닝모델 등을 개발할때나 이 코드 다시 볼거임.
                        ->  이건 딥러닝 프로그램을 밑바닥부터 만다는 코드일뿐. 실무는 이거보다 쉬움. 가볍게 보고 넘어가기 

    1. 예측 모델 만들기
    2. optimizer, 손실함수 정하고
    3. 학습하기 (경사하강으로 변수값 업데이트하기)

    ----디테일한 순서-----------
    1. 예측 모델 만들기 (
        -예측모델식 만들기
        -Variable 만들기 
        )
    
    2. 학습시키기  (
        -옵티마이저 정해서 만들기 - Adam()등,  learning_rate정해서 넣기 - 결과 잘나올때까지 수정필요함
        -손실함수 정해서 만들기 (손실함수란 - 모델을 통한 예측값과 실제값의 차이, 즉 오차를 반환) 
                     (예측모델에 따라 쓸 수 있는 손실함수는 여러가지가 있음. 지금처럼 정수를 예측하고 싶다면 mean squared error쓰고 카테고리분류문제 or 확률예측문제 등은 cross entropy 이용)
        -학습하기 - 반복문으로경사하강법 진행 (손실함수, 업데이트해줄 w값들 인자로 넣어서)

        )
    
"""

# 보통 이렇게 w값인 변수의 인자에 들어가는 초기화 값은 Randomize함. 무작위로 넣음. 
a = tf.Variable(0.1) 
b = tf.Variable(0.1)

예측_y = train_x * a + b  # 예측모델임. 이제 컴퓨터한테 a, b뭔지 추론해보라고 하는것임. 
                         # 만약 2차함수 3차함수 문제면 여기에 제곱이 들어가고 할거임. 근데 이 문제는 직선문제로 해결될거같아서 이렇게 모델 정한것

#mean squared error를 이용해서 만든 손실함수 //  
#mean squared error - 데이터가 1개일때 : (예측값 - 실제값)^2 을 해서 오차값 구함
#                   - 데이터가 여러개일때:  (예측값1 - 실제값1)^2 + (예측값2 - 실제값2)^2 + ....  / 7    (여러개의 데이터들을 다 더해서 평균을 냄)
#                   - tf.keras.losses.mse(실제값, 예측값)   // 텐서플로우에 있는 이 함수 이용하면 쉽게 mean squared error값 구할 수 있음

def 손실함수(a,b):         #손실함수의 인자로 변수값 안넣어도 돌아가지만 이게 보기좋고 현업에서 이렇게 많이 쓰는듯
    예측_y = train_x * a + b  # !! 근데 train_x는 여러 데이터 들어있는 리스트인건데 거기에 곱하기하고 더하기 되나?? -> 텐서플로우에선 됨. 행렬처럼 생각해서 각각 데이터에 곱해주고 더해줌. 
    return tf.keras.losses.mse(train_y, 예측_y)  #실제값, 예측값 순서 안틀리게 조심.  !! 여러 데이터를 바탕으로 오차구하는거니까 실제값List와 예측값List를 넣어줌. 이러면 한번에 다 loss값 계산해줌.


#옵티마이저 원하는걸로 정해주고 learning_rate도 정해주고 싶으면 넣기
opt = tf.keras.optimizers.Adam(learning_rate=0.01)  


for i in range(300):
    opt.minimize( lambda: 손실함수(a,b), var_list=[a,b] )   #첫번째 인자값은 함수명만 들어가야함. 그래서 손실함수(a,b) 이렇게 쓰면 안됨. 그래서 걍 파이썬 문법인 lambda를 써줌. 원하는 함수를 큰 익명함수로 감싼거임. 중요x
    print(a.numpy(),b.numpy())    


# !! 여기서 우리가 예상하는 값은 a는 2이고 b는 1임. 근데 원하는 값에 한참 못미친 출력 결과가 나오면 반복문 횟수를 증가시키거나 learning_rate값을 증가시키면서 해보셈 
# 어떻게 보면 때려맞추기지만 그게 딥러닝 돌리는거의 핵심임.


#위의 과정들은 사실 딥러닝은 아님. 왜나면 딥러닝은 만든 뉴럴 네트워크를 타고 생각의 과정을 거쳐서 결과를 도출하는 것임.. 방금 한건 그냥 learning임. 
# 딥러닝을 만드려면 중간 hidden layer들이 있어야함.
# (hidden layer 계산하는 방식은 w값들과 인풋값들 곱하고 더해서 node를 만들고.. 그 노드를 인풋삼고 w를 통해 또 한번 연산을 해서.. 최종 예측값을 냄) 

# 즉, 예측_y = train_x * a + b   이 최종 예측값 식인 '예측_y'을 만드는 걸 뉴럴 네트워크식으로 연산하자. 이걸 딥러닝이라고 함. 
# 이런거 행렬 잘하면 잘할 수 있음. 이런 모델을 어떻게 실제로 계산할 수 있는지 궁금하면 집에서 직접 해보고, 이런거 필요없고 딥러닝하는 법만 배울거면 다음 강의로