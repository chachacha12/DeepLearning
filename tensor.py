import tensorflow as tf

# 딥러닝으로 간단한 수학문제 풀어보기


"""
키 = [170, 180, 175, 160]
신발 = [260, 270,265, 255]

문제 : 키와 신발사이즈는 어떤 관련이 있을까요?

y = ax+b  (y는 신발사이즈이고 x는 키값... 모르는 a,b값을 찾는게 목표)

이거와 유사한 작업을 딥러닝으로 구현해볼거임

"""

# 문제: 키로 신발사이즈를 추론해보자

키 = 170
신발 = 260

# 신발 = 키 * a + b  # 문제해결을 위해 만들어본 식임

a = tf.Variable(0.1)
b = tf.Variable(0.2)


#손실함수란? - 별거아니고 실제값과 예측값의 차를 mean squared error를 쓰거나 binary cross entropy 등을 써서 퉤 뱉어주면 되는 함수임

#뒤에서 경사하강법 실행할때 필요한 손실함수 만들어줘야함. 손실함수는 return으로 손실값(실제값 - 예측값)..즉 오차를 뱉어주면됨. 
# 근데 너무 마이너스값으로 치닫는거 방지하고자 제곱해서 뱉어줌. 제곱은 텐서플로우에 tf.square()함수 쓰면됨
#  tf.square(실제값 - 예측값) 이거말고도 원하는 다른 여러가지 종류의 손실값들을 return 뒤에 넣어주면 됨..
# 여기서 실제값은 신발값인 260일거고, 예측값은  키 * a + b  이거일거임. 즉  tf.square(260 - ( 키 * a + b)) 이렇게 넣어줌
def 손실함수():
    예측값 =  키 * a + b 
    return tf.square(260 - 예측값)


# 이제 a,b 좋은결과 나올때까지 딥러닝으로 학습을 시켜볼거임

# tf.keras.optimizers   //이건 알아서 경사하강법해서 a, b값 업데이트해주는 고마운 함수


#optimizer라는 변수 생성
opt = tf.keras.optimizers.Adam(learning_rate=0.1)  #optimizers다음에 여러가지 optimizers함수들 쓸 수 있는데 Adam()이란걸 써봄. 경우에 따라서 w값을 많이 업데이트해주거나 적게 업데이트해줌.
                            #즉, gradient를 알아서 스마트하게 바꿔주는 optimizer임. 얘 말고도 여러가지 있는데 전반적으로 얘가 성능좋음. 
                            # Adam()의 인자값으로 learning_rate를 줄 수도 있음. 안줘도 디폴트로 0.0001인가 값 들어감. 


for i in range(300):
    opt.minimize( 손실함수, var_list=[a,b] )   #이러면 경사하강 1번 실행해줌.  그러면 w값인 a와 b Variable들이 바뀐 값으로 1번 업뎃됨
    print(a.numpy(),b.numpy())               #이걸 여러번 실행해주면 최적의 a,b값 찾을 수 있고 그게 딥러닝임.
                                             #인자 2개 필수로 넣어줘야하는데
                                             #첫번째 인자 : loss function(손실함수) 만들어서 넣어줘야함
                                             #두번째 인자 : 경사하강법으로 업데이트할 weight Variable 목록들 넣어줌


"""
위 방법을 쓰면 a값은 1.52정도 나오고 b는 1.62정도 나옴. 이걸 아까 일차식에 넣으면

예측한 신발사이즈 = 키 * 1.52 + 1.62

이렇게 나옴. 이게 우리가 만든 ( Linear Regression ) 모델임.
이제 키값만 알면 그 사람의 신발사이즈를 추정할 수 있는거임.

--> 근데 오늘한 이런 방법은 사실 딥러닝 원리 시범한거 일뿐, 실제 프로젝트는 더 쉽고 이렇게 안돌림.

    @ 이제 다음시간에 할 건 키랑 신발 데이터가 훨씬 많으면 그건 어떻게 추론할지 해볼건데.. 이것까지 원리 시범하는거 일듯. 실제 프로젝트는 그렇게 하진 않을거임.

오늘거랑 다음시간에 하는것까진 가벼운 마음으로 듣고 넘어가도될듯.

"""